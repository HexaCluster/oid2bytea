#!/usr/bin/perl
#------------------------------------------------------------------------------
# Project  : This script is allow migration of large object column into bytea.
# Language : Perl
# Authors  : Gilles Darold, gilles _AT_ darold _DOT_ net
# Copyright: Copyright (c) 2025 : HexaCluster - All rights reserved -
# Usage    : See oid2bytea --help
# Licence  : PostgreSQL
#------------------------------------------------------------------------------
use strict;

use Getopt::Long  qw(:config bundling no_ignore_case_always);
use POSIX qw(locale_h sys_wait_h _exit);
use Time::HiRes qw/usleep/;
use DBI;
use DBD::Pg;
use POSIX qw(strftime);

$| = 1;

my $VERSION = '1.0';

# Command line options variables
my $DBNAME = '';
my $DBUSER = '';
my $DBHOST = '';
my $DBPORT = 5432;
my $DBPWD = $ENV{PGPASSWORD} || '';
my $VER    = 0;
my $HELP   = 0;
my $QUIET  = 0;
my $JOBS   = 1;
my $SPLIT  = 1;
my $NOTIME = 0;
my $DRY_RUN    = 0;
my $NOVACUUMLO = 0;
my $FUNCTION   = '';
my $NODROP     = 0;
my $MISSING    = 0;

my @INC_TABLES  = ();
my @INC_SCHEMAS = ();

my $REMOTE_DBNAME = '';
my $REMOTE_DBUSER = '';
my $REMOTE_DBHOST = '';
my $REMOTE_DBPORT = 5432;
my $REMOTE_DBPWD = $ENV{REMOTE_PGPASSWORD} || $ENV{PGPASSWORD} || '';

my $interrupt    = 0;
my $child_count  = 0;
my %RUNNING_PIDS = ();
my $PG_OPT       = '';
my $REMOTE_PG_OPT = '';
my $dbh          = undef;

my $parent_pid = $$;
my $TMP_LOG_FILE = '/tmp/oid2bytea.tmp';

####
# Method used to fork as many child as wanted,
# must be declared at top if the program.
####
sub spawn
{
	my $coderef = shift;

	unless (@_ == 0 && $coderef && ref($coderef) eq 'CODE')
	{
		print "usage: spawn CODEREF";
		exit 0;
	}

	my $pid;
	if (!defined($pid = fork)) {
		print STDERR "Error: cannot fork: $!\n";
		return;
	} elsif ($pid) {
		$RUNNING_PIDS{$pid} = $pid;
		return; # the parent
	}
	# the child -- go spawn
	$< = $>;
	$( = $); # suid progs only

	exit &$coderef();
}

# Child reports error
sub child_error
{
	my $sig = shift;

	$interrupt = 1;

	print STDERR "An error occurs from a child process, aborting...\n";
	if ($^O !~ /MSWin32|dos/i) {
		1 while wait != -1;
		$SIG{INT} = \&wait_child;
		$SIG{TERM} = \&wait_child;
		$SIG{USR1} = \&child_error;
	}
	_exit(1);
}

# With multiprocess we need to wait for all children
sub wait_child
{
	my $sig = shift;

	$interrupt = 1;

	print STDERR "Received terminating signal ($sig)\n";
	if ($^O !~ /MSWin32|dos/i) {
		1 while wait != -1;
		$SIG{INT} = \&wait_child;
		$SIG{TERM} = \&wait_child;
	}
	$dbh->disconnect() if (defined $dbh);

	_exit(0);
}
$SIG{INT} = \&wait_child;
$SIG{TERM} = \&wait_child;
$SIG{USR1} = \&child_error;

$| = 1;

GetOptions(
	"d|database=s"       => \$DBNAME,
	"D|remote-database=s"=> \$REMOTE_DBNAME,
	"f|function=s"       => \$FUNCTION,
	"h|host=s"           => \$DBHOST,
	"H|remote-host=s"    => \$REMOTE_DBHOST,
	"j|jobs=i"           => \$JOBS,
	"J|job-split=i"      => \$SPLIT,
	"m|missing!"         => \$MISSING,
	"n|namespace=s"      => \@INC_SCHEMAS,
	"p|port=i"           => \$DBPORT,
	"P|remote-port=i"    => \$REMOTE_DBPORT,
	"q|quiet!"           => \$QUIET,
	"t|table=s"          => \@INC_TABLES,
	"u|user=s"           => \$DBUSER,
	"U|remote-user=s"    => \$REMOTE_DBUSER,
	"v|version!"         => \$VER,
	"V|no-vacuumlo!"     => \$NOVACUUMLO,
	"no-drop!"           => \$NODROP,
	"no-time!"           => \$NOTIME,
	"dry-run!"           => \$DRY_RUN,
	"help!"              => \$HELP,
);

if ($VER)
{
	print "oid2bytea Version: v$VERSION\n";
	exit 0;
}

&usage if ($HELP);

# verify that vacuumlo is available in the path
if (!$NOVACUUMLO)
{
	my $vacuumlo_ver = `vacuumlo -V 2> /dev/null`;
	chomp($vacuumlo_ver);
	if (!$vacuumlo_ver) {
		die("Command vacuumlo must be installed and available from the PATH environment variable\n");
	}
}

if (!$DBNAME) {
        &usage("ERROR: you must specify a database, see -d option\n");
}

# Set PostgreSQL related options
if ($DBHOST) {
	$PG_OPT .= " -h $DBHOST";
}
if ($DBPORT) {
	$PG_OPT .= " -p $DBPORT";
}
if ($DBUSER) {
	$PG_OPT .= " -u $DBUSER";
}

# Used to serialize write to stdout output
my $fh = new IO::File;
$fh->open(">$TMP_LOG_FILE") or die "FATAL: can't write to temp file $TMP_LOG_FILE, $!\n";
$fh->print();
$fh->close();

# Set remote PostgreSQL related options
if ($REMOTE_DBNAME) {
	$REMOTE_PG_OPT .= " -d $REMOTE_DBNAME";
}
if ($REMOTE_DBHOST) {
	$REMOTE_PG_OPT .= " -h $REMOTE_DBHOST";
}
if ($REMOTE_DBPORT) {
	$REMOTE_PG_OPT .= " -p $REMOTE_DBPORT";
}
if ($REMOTE_DBUSER) {
	$REMOTE_PG_OPT .= " -U $REMOTE_DBUSER";
}

# Set DBD::Pg options and connect to the database for testing
my $dbpg_opt = '';
$dbpg_opt .= ";port=$DBPORT" if ($DBPORT);
$dbpg_opt .= ";host=$DBHOST" if ($DBHOST);
$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
if (not defined $dbh) {
	logmsg('FATAL', "cannot connect to database $DBNAME");
}

my $remote_dbpg_opt = '';
$remote_dbpg_opt .= ";port=$REMOTE_DBPORT" if ($REMOTE_DBPORT);
$remote_dbpg_opt .= ";host=$REMOTE_DBHOST" if ($REMOTE_DBHOST);

# Look at PostgreSQL version
my $sth = $dbh->prepare("SELECT version()") or die "FATAL: " . $dbh->errstr . "\n";
$sth->execute or die "FATAL: " . $dbh->errstr . "\n";
my $pgversion = 0;
while (my $row = $sth->fetch)
{
	if ($row->[0] =~ /^[^\s]+ ([\.\d]+)/) {
		$pgversion = $1;
	}
}
$sth->finish;
logmsg('LOG', "Connection to PostgreSQL successfull");

logmsg('LOG', 'Starting conversion of large objects to bytea');

# Get oid columns to migrate
logmsg('LOG', 'Looking for large objects columns in relations');
my $collist_query = "SELECT n.nspname, c.relname, a.attname, c.relkind, c.relispartition FROM pg_attribute a JOIN pg_class c ON (a.attrelid=c.oid) JOIN pg_namespace n ON (c.relnamespace=n.oid) WHERE a.atttypid = 26 AND a.attnum > 0 AND c.relkind IN ('r', 'p')";
if ($#INC_SCHEMAS < 0)
{
	$collist_query .= " AND n.nspname NOT IN ('pg_catalog', 'pg_toast', 'information_schema') AND NOT EXISTS (SELECT 1 FROM pg_catalog.pg_depend d WHERE d.refclassid = 'pg_catalog.pg_extension'::pg_catalog.regclass AND d.objid = c.oid AND d.deptype = 'e')";
}
else
{
	$collist_query .= "  AND n.nspname IN ('" . join("','", @INC_SCHEMAS) . "')";
}
$collist_query .= " ORDER BY a.attnum";

$sth = $dbh->prepare($collist_query) or die "FATAL: " . $dbh->errstr . "\n";
$sth->execute or die "FATAL: " . $dbh->errstr . "\n";

# Query to get the full coilumn list of a table
my $allcol_query = "SELECT a.attname FROM pg_attribute a JOIN pg_class c ON (a.attrelid=c.oid) JOIN pg_namespace n ON (c.relnamespace=n.oid) WHERE n.nspname = ? AND c.relname = ? AND a.attnum > 0 ORDER BY a.attnum";
my $sth2 = $dbh->prepare($allcol_query) or die "FATAL: " . $dbh->errstr . "\n";

my %collist = ();
my %oid_collist = ();
my %relkind = ();
while (my $row = $sth->fetch)
{
	logmsg('LOG', "Found oid column in table $row->[0].$row->[1] column $row->[2].");
	if ($#INC_TABLES < 0 || grep(/^$row->[1]$/, @INC_TABLES) || grep(/^$row->[0].$row->[1]$/, @INC_TABLES))
	{
		logmsg('LOG', "Registering column \"$row->[2]\" from table \"$row->[0].$row->[1]\" for migration.");
		push(@{ $oid_collist{"$row->[0].$row->[1]"} }, $row->[2]);
		$relkind{"$row->[0].$row->[1]"}{kind} = $row->[3];
		$relkind{"$row->[0].$row->[1]"}{ispartition} = $row->[4];
		# Grab the full list of column for remote work
		if ($allcol_query)
		{
			$sth2->execute($row->[0], $row->[1]) or die "FATAL: " . $dbh->errstr . "\n";
			while (my $r = $sth2->fetch)
			{
				push(@{ $collist{"$row->[0].$row->[1]"} }, $r->[0]);
			}
		}
	}
	else
	{
		logmsg('LOG', "Skiping, not in table list.");
	}
}
$sth->finish;
$sth2->finish;

# Disconnect from snapshot connection
$dbh->disconnect;

if (scalar keys %oid_collist == 0) {
	logmsg('LOG', "No large object columns found.");
}

if ($MISSING)
{
	foreach my $tbname (sort keys %collist)
	{
		verif_transform($tbname, @{ $oid_collist{$tbname} });
		last if ($interrupt);
	}
	unlink($TMP_LOG_FILE);
	exit 0;
}

my $procnum = 0;
if (!$REMOTE_DBNAME)
{
	foreach my $tbname (sort keys %collist)
	{
		pre_transform($procnum, $tbname, @{ $oid_collist{$tbname} });
		last if ($interrupt);
	}

}

foreach my $tbname (sort keys %collist)
{
	last if ($interrupt);

	spawn sub {
		if (!$REMOTE_DBNAME) {
			logmsg('LOG', "Processing table $tbname locally...");
			transform_oid_to_bytea($procnum, $tbname, @{ $oid_collist{$tbname} });
		} elsif ($relkind{$tbname} ne 'p') {
			logmsg('LOG', "Processing table $tbname remotely...");
			remote_transform_oid_to_bytea($procnum, $tbname, \@{ $collist{$tbname} }, \@{ $oid_collist{$tbname} });
		}
	};
	$procnum++;

	while ($procnum >= $JOBS)
	{
		my $kid = waitpid(-1, WNOHANG);
		if ($kid > 0)
		{
			$procnum--;
			delete $RUNNING_PIDS{$kid};
			last;
		}
		usleep(50000);
	}
}

# Then wait for all child processes to die
while (scalar keys %RUNNING_PIDS > 0)
{
        my $kid = waitpid(-1, WNOHANG);
        if ($kid > 0)
        {
                delete $RUNNING_PIDS{$kid};
        }
        usleep(50000);
}

if (!$REMOTE_DBNAME)
{
	foreach my $tbname (sort keys %collist)
	{
		post_transform($procnum, $tbname, @{ $oid_collist{$tbname} });
		last if ($interrupt);
	}
}

# Now vacuum all orphan large object from the database
if (scalar keys %oid_collist > 0)
{
	logmsg('LOG', "Running vacuumlo on the database to remove orphan large object");
	if ($DRY_RUN) {
		logmsg('LOG', "vacuumlo --dry-run $PG_OPT $DBNAME");
	} else {
		`vacuumlo $PG_OPT $DBNAME`;
	}
}

if ($interrupt)
{
	logmsg('LOG', '');
	unlink($TMP_LOG_FILE);
	_exit(1);
}

logmsg('LOG', 'convertion ended');

unlink($TMP_LOG_FILE);

exit 0;

#---------------------------------------------------------------------

####
# Show help about the program
####
sub usage
{
        my $msg = shift();

        print qq{
Program used to convert large objects columns in a PostgreSQL database into
bytea. Two migration modes are supported: local and remote.

**local**: the corresponding large objects data stored in the pg_largeobject table
will be moved into a newly created bytea column and the old oid column will be
removed. If no table list to convert is provided all columns with the oid data
type will be converted to bytea otherwise all oid columns of the table list
will be converted to bytea. 

	ALTER TABLE tb1 ADD COLUMN bytea_col bytea;
	UPDATE tb1 SET bytea_col = lo_get(lo_col);
	UPDATE tb1 AS t1 SET bytea_col = lo_get(lo_col)
		FROM pg_largeobject_metadata t2
		WHERE t2.oid = t1.lo_col;
	ALTER TABLE tb1 DROP COLUMN lo_col;
	ALTER TABLE tb1 RENAME COLUMN bytea_col TO lo_col;

Once all large objects are migrated the tool runs the vacuumlo command to
remove all orphan large objects from the database.

Note that all missing large object will not report error due to the JOIN in the
update statement but you can find them look for NULL in the bytea column.

WARNING: the large object data will be duplicated until the `vacuumlo` command
is run, be sure to have enough free space.

You may want to run a VACUUM FULL on modified tables to recover space from
the oid column(s).

Also take care in your application that the bytea column(s) are appended at end
of the table so their attribute position number change.

**remote**:
in this mode oid2bytea will migrate the local oid column to a remote database
with the same structure except that the oid column have been replaced by a bytea
column of the same name. To use the remote mode you just have to set the remote
command line options (-D, -H, -P, -U). The all the data of the table with the oid
column(s) will be moved and the oid replaced by the content of the large object
before being sent to the remote database.

Note that missing large objects will throw an error and the data migration will
be aborted for the table. To verify first if the table has no missing large
objects run the command with the --missing option.

**multiprocess**:
The two modes can benefit of high speed migration by parallelizing tables
migration using the `-j` option. With table with huge amount of rows it is also
possible to parallelize rows export of a single table by using the `-J` option.
The value for these option is the number of processes/CPUs you want to use to
process the data.  Take care that the resulting number of processes/CPUs used
is `-j * -J`.

**transform function**
Using the -f option it is possible to use a function to process the data before
inserting in the new column. For example, if all your large objects data have
been compressed using gzip, you may want to uncompress the bytea data to benefit
of the PostgreSQL native toast compression. In this case, use the psql-gzip
(https://github.com/pramsey/pgsql-gzip) extension and use -f gunzip at command
line.

Another example of use is to convert the large objects to a text column if all
the data are text data. In this case use -f "encode(%, 'escape')" at command
line and oid2bytea will replace the % placeholder by the call to lo_get() with
the name of the column processed.

Usage: oid2bytea -d dbname [options]

options:

  -d, --database DBNAME      database where the migration job must be done.
  -D, --dest-database DBNAME database where the migration data must be sent.
  -f, --function FCTNAME     use this function to process the bytea returned
                             by the call to lo_get(). It must return a bytea
			     or the data type of the destination column.
  -h, --host HOSTNAME        database server host or socket directory.
  -H, --dest-host HOSTNAME   remote database server host or socket directory.
  -j, --job NUM              use this many parallel jobs to process all tables.
  -J, --job-split NUM        use this many parallel jobs to process single table
  -m, --missing              show missing large objects for a table and do not
                             perform any migration.
  -n, --namespace NAME       process tables created under the given schema.
                             Can be used multiple time.
  -p, --port PORT            database server port number, default: 5432.
  -P, --dest-port PORT       database server port number, default: 5432.
  -q, --quiet                don't display messages issued with the LOG level.
  -t, --table TABLE          migrate oid column(s) of the named relation. Can
                             be used multiple time.
  -u, --user NAME            connect as specified database user.
  -U, --dest-user NAME       remote connect as specified database user.
  -v, --version              show program version.
  -V, --no-vacuumlo          do not run vacuumlo at end to remove orphan large
                             objects.
  --help                     show usage.
  --no-drop                  don't drop the oid column, rename it to oid_colN
                             instead. N depends on the number of the oid column.
  --no-time                  don't show timestamp in log output.
  --dry-run                  do not do anything, just show what will be done.

$msg
};
	exit 0;
}

####
# Return the date in ISO 8601 format minus the timezone part.
# As an example of returned value: 2019-09-03T09:03:12
####
sub get_current_date
{
    return strftime("%FT%T", localtime);
}

####
# Print a message with date/time and log level.
####
sub logmsg
{
	my ($level, $message) = @_;

	return if ($QUIET);

	$fh = new IO::File;
	$fh->open(">>$TMP_LOG_FILE") or die "FATAL: can't write to temp file $TMP_LOG_FILE, $!\n";
	flock($fh, 2) || die "FATAL: can't lock file $TMP_LOG_FILE\n";

	if ($NOTIME) {
		print "$level: $message\n";
	} else {
		print '[' , get_current_date(), '] ', "$level: $message\n";
	}

	$fh->close;
}

sub pre_transform
{
	my ($proc, $tb, @colums) = @_;

	return if ($#colums < 0);

	# ALTERs will only be executed on root table
	return if ($relkind{$tb}{ispartition});

	logmsg('LOG', "Adding bytea column to table $tb locally...");

	$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
	if (not defined $dbh) {
		logmsg('FATAL', "cannot connect to database $DBNAME");
	}

	# Construct the SQL orders to execute
	my @alter = ();
	for (my $c = 0; $c <= $#colums; $c++)
	{
		push(@alter, "ALTER TABLE $tb ADD COLUMN bytea_col$c bytea");
	}

	# Add the bytea column(s)
	foreach my $q (@alter)
	{
		logmsg('LOG', $q);
		if (!$DRY_RUN) {
			$dbh->do($q) or logmsg('ERROR', $dbh->errstr);
		}
	}

	$dbh->disconnect();
}

sub verif_transform
{
	my ($tb, @colums) = @_;

	return if ($#colums < 0);

	# ALTERs will only be executed on root table
	return if ($relkind{$tb}{ispartition});


	$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
	if (not defined $dbh) {
		logmsg('FATAL', "cannot connect to database $DBNAME");
	}

	# Construct the SQL orders to execute
	my @alter = ();
	for (my $c = 0; $c <= $#colums; $c++)
	{
		logmsg('LOG', "Verifying if column $tb.$colums[$c] has missing large objects that will make lo_get() fail...");
		my $q = "SELECT t1.$colums[$c] FROM $tb t1 WHERE NOT EXISTS (SELECT 1 FROM pg_largeobject_metadata WHERE oid = t1.$colums[$c]);";
		my $s = $dbh->prepare($q) or die "FATAL: " . $dbh->errstr . "\n";
		$s->execute or die "FATAL: " . $dbh->errstr . "\n";
		my @missing = ();
		while (my $row = $s->fetch)
		{
			push(@missing, $row->[0]);
		}
		$s->finish;
		if ($#missing >= 0) {
			logmsg('LOG', "  The following oids are missing: " . join(',', @missing));
		} else {
			logmsg('LOG', "  No oids are missing");
		}
	}

	$dbh->disconnect();
}

sub transform_oid_to_bytea
{
	my ($proc, $tb, @colums) = @_;

	return if ($#colums < 0);

	# Update will not be executed on partitioned table, only relation
	return if ($relkind{$tb}{kind} eq 'p');

	$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
	if (not defined $dbh) {
		logmsg('FATAL', "cannot connect to database $DBNAME");
	}

	# Construct the SQL orders to execute
	for (my $c = 0; $c <= $#colums; $c++)
	{
		my $update_sql = "UPDATE $tb AS t1 SET bytea_col$c = ";
		if ($FUNCTION && $FUNCTION !~ /\%/) {
			$update_sql .= "$FUNCTION(lo_get($colums[$c])) ";
		} elsif ($FUNCTION && $FUNCTION =~ /\%/) {
			my $fct = $FUNCTION;
			$fct =~ s/\%/lo_get($colums[$c])/g;
			$update_sql .= "$fct ";
		} else {
			$update_sql .= "lo_get($colums[$c]) ";
		}
		$update_sql .= " FROM pg_largeobject_metadata t2 WHERE t2.oid = t1.$colums[$c]";

		# Migrate the large object into the bytea column(s).
		if ($SPLIT > 1)
		{
			my $procnum = 0;
			for my $splitnum (1 .. $SPLIT)
			{
				logmsg('LOG', "Processing table $tb on chunk $splitnum on column $colums[0]...");
				spawn sub {
					update_column($update_sql, "t1.$colums[$c]", $splitnum);
				};
				$procnum++;
			}
			while ($procnum > 0)
			{
				my $kid = waitpid(-1, WNOHANG);
				if ($kid > 0)
				{
					$procnum--;
					delete $RUNNING_PIDS{$kid};
					last;
				}
				usleep(50000);
			}
		}
		else
		{
			update_column($update_sql, '', 0);
		}
	}

	$dbh->disconnect();
}

sub post_transform
{
	my ($proc, $tb, @colums) = @_;

	return if ($#colums < 0);

	# ALTERs will only be executed on root table
	return if ($relkind{$tb}{ispartition});

	my $action = 'Renaming';
	$action = 'Dropping' if (!$NODROP);
	logmsg('LOG', "$action oid column and renaming bytea column in table $tb locally...");

	$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
	if (not defined $dbh) {
		logmsg('FATAL', "cannot connect to database $DBNAME");
	}

	# Construct the SQL orders to execute
	my @alter = ();
	for (my $c = 0; $c <= $#colums; $c++)
	{
		if ( !$NODROP ) {
			push(@alter, "ALTER TABLE $tb DROP COLUMN $colums[$c]");
		} else {
			push(@alter, "ALTER TABLE $tb RENAME COLUMN $colums[$c] TO oid_col$c");
		}
		push(@alter, "ALTER TABLE $tb RENAME COLUMN bytea_col$c TO $colums[$c]");
	}

	# Drop the oid column(s) and rename the bytea column(s) with the old name of the old oid column(s)
	foreach my $q (@alter)
	{
		logmsg('LOG', $q);
		if (!$DRY_RUN) {
			$dbh->do($q) or logmsg('ERROR', $dbh->errstr);
		}
	}

	$dbh->disconnect();
}


sub remote_transform_oid_to_bytea
{
	my ($proc, $tb, $all_columns, $oid_colums) = @_;

	return if ($#{$oid_colums} < 0);

	my $query_get = '';
	foreach my $c (@$all_columns)
	{
		if (!grep(/^$c$/, @$oid_colums)) {
			$query_get .= "$c, ";
		} else {
			if ($FUNCTION && $FUNCTION !~ /\%/) {
				$query_get .= "$FUNCTION(lo_get($c)), ";
			} elsif ($FUNCTION && $FUNCTION =~ /\%/) {
				my $fct = $FUNCTION;
				$fct =~ s/\%/lo_get($c)/g;
				$query_get .= "$fct, ";
			} else{
				$query_get .= "lo_get($c), ";
			}
		}
	}
	$query_get =~ s/, $//;
	$query_get .= ' FROM ' . $tb;

	$query_get .= " WHERE MOD($oid_colums->[0]::bigint, $SPLIT) = $procnum - 1" if ($procnum > 0 and $oid_colums->[0]);
	$query_get = qq{COPY (SELECT $query_get) TO PROGRAM 'psql -X $REMOTE_PG_OPT -c "\\\\copy $tb FROM stdin"'};

	# Construct the SQL query to get data
	if ($SPLIT > 1)
	{
		my $procnum = 0;
		for my $splitnum (1 .. $SPLIT)
		{
			logmsg('LOG', "Processing table $tb on chunk $splitnum on column $oid_colums->[0]...");
			spawn sub {
				forward_data($query_get, $splitnum);
			};
			$procnum++;
		}
		while ($procnum > 0)
		{
			my $kid = waitpid(-1, WNOHANG);
			if ($kid > 0)
			{
				$procnum--;
				delete $RUNNING_PIDS{$kid};
				last;
			}
			usleep(50000);
		}
	}
	else
	{
		forward_data($query_get, 0);
	}
}


sub update_column
{
	my ($update_sql, $split_col, $procnum) = @_;

	$update_sql .= " AND MOD(${split_col}::bigint, $SPLIT) = $procnum - 1" if ($procnum > 0 and $split_col);
	logmsg('LOG', $update_sql);
	if (!$DRY_RUN)
	{
		$dbh->do($update_sql) or logmsg('ERROR', $dbh->errstr);
	}
}

sub forward_data
{
	my ($sql_get, $procnum) = @_;

	logmsg('LOG', $sql_get);
	if (!$DRY_RUN)
	{
		my $dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
		if (not defined $dbh) {
			logmsg('FATAL', "cannot connect to database $DBNAME");
		}
		$dbh->do($sql_get);
		$dbh->disconnect();
	}
}
