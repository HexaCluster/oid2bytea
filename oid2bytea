#!/usr/bin/perl
#------------------------------------------------------------------------------
# Project  : This script is allow migration of large object column into bytea.
# Language : Perl
# Authors  : Gilles Darold, gilles _AT_ darold _DOT_ net
# Copyright: Copyright (c) 2025 : HexaCluster - All rights reserved -
# Usage    : See oid2bytea --help
# Licence  : PostgreSQL
#------------------------------------------------------------------------------
use strict;

use Getopt::Long  qw(:config bundling no_ignore_case_always);
use POSIX qw(locale_h sys_wait_h _exit);
use Time::HiRes qw/usleep/;
use DBI;
use DBD::Pg;
use POSIX qw(strftime);

$| = 1;

my $VERSION = '1.0';

# Command line options variables
my $DBNAME = '';
my $DBUSER = '';
my $DBHOST = '';
my $DBPORT = 5432;
my $DBPWD = $ENV{PGPASSWORD} || '';
my $VER    = 0;
my $HELP   = 0;
my $QUIET  = 0;
my $JOBS   = 1;
my $SPLIT  = 1;
my $NOTIME = 0;
my $DRY_RUN    = 0;
my $NOVACUUMLO = 0;
my $FUNCTION = '';

my @INC_TABLES  = ();
my @INC_SCHEMAS = ();

my $REMOTE_DBNAME = '';
my $REMOTE_DBUSER = '';
my $REMOTE_DBHOST = '';
my $REMOTE_DBPORT = 5432;
my $REMOTE_DBPWD = $ENV{REMOTE_PGPASSWORD} || $ENV{PGPASSWORD} || '';

my $interrupt    = 0;
my $child_count  = 0;
my %RUNNING_PIDS = ();
my $PG_OPT       = '';
my $REMOTE_PG_OPT = '';
my $dbh          = undef;

my $parent_pid = $$;
my $TMP_LOG_FILE = '/tmp/oid2bytea.tmp';

####
# Method used to fork as many child as wanted,
# must be declared at top if the program.
####
sub spawn
{
	my $coderef = shift;

	unless (@_ == 0 && $coderef && ref($coderef) eq 'CODE')
	{
		print "usage: spawn CODEREF";
		exit 0;
	}

	my $pid;
	if (!defined($pid = fork)) {
		print STDERR "Error: cannot fork: $!\n";
		return;
	} elsif ($pid) {
		$RUNNING_PIDS{$pid} = $pid;
		return; # the parent
	}
	# the child -- go spawn
	$< = $>;
	$( = $); # suid progs only

	exit &$coderef();
}

# Child reports error
sub child_error
{
	my $sig = shift;

	$interrupt = 1;

	print STDERR "An error occurs from a child process, aborting...\n";
	if ($^O !~ /MSWin32|dos/i) {
		1 while wait != -1;
		$SIG{INT} = \&wait_child;
		$SIG{TERM} = \&wait_child;
		$SIG{USR1} = \&child_error;
	}
	_exit(1);
}

# With multiprocess we need to wait for all children
sub wait_child
{
	my $sig = shift;

	$interrupt = 1;

	print STDERR "Received terminating signal ($sig)\n";
	if ($^O !~ /MSWin32|dos/i) {
		1 while wait != -1;
		$SIG{INT} = \&wait_child;
		$SIG{TERM} = \&wait_child;
	}
	$dbh->disconnect() if (defined $dbh);

	_exit(0);
}
$SIG{INT} = \&wait_child;
$SIG{TERM} = \&wait_child;
$SIG{USR1} = \&child_error;

$| = 1;

GetOptions(
	"d|database=s"       => \$DBNAME,
	"D|remote-database=s"=> \$REMOTE_DBNAME,
	"f|function=s"       => \$FUNCTION,
	"h|host=s"           => \$DBHOST,
	"H|remote-host=s"    => \$REMOTE_DBHOST,
	"j|jobs=i"           => \$JOBS,
	"J|job-split=i"      => \$SPLIT,
	"n|namespace=s"      => \@INC_SCHEMAS,
	"p|port=i"           => \$DBPORT,
	"P|remote-port=i"    => \$REMOTE_DBPORT,
	"q|quiet!"           => \$QUIET,
	"t|table=s"          => \@INC_TABLES,
	"u|user=s"           => \$DBUSER,
	"U|remote-user=s"    => \$REMOTE_DBUSER,
	"v|version!"         => \$VER,
	"V|no-vacuumlo!"     => \$NOVACUUMLO,
	"no-time!"           => \$NOTIME,
	"dry-run!"           => \$DRY_RUN,
	"help!"              => \$HELP,
);

if ($VER)
{
	print "oid2bytea Version: v$VERSION\n";
	exit 0;
}

&usage if ($HELP);

# verify that vacuumlo is available in the path
if (!$NOVACUUMLO)
{
	my $vacuumlo_ver = `vacuumlo -V 2> /dev/null`;
	chomp($vacuumlo_ver);
	if (!$vacuumlo_ver) {
		die("Command vacuumlo must be installed and available from the PATH environment variable\n");
	}
}

if (!$DBNAME) {
        &usage("ERROR: you must specify a database, see -d option\n");
}

# Set PostgreSQL related options
if ($DBHOST) {
	$PG_OPT .= " -h $DBHOST";
}
if ($DBPORT) {
	$PG_OPT .= " -p $DBPORT";
}
if ($DBUSER) {
	$PG_OPT .= " -u $DBUSER";
}

my $fh = new IO::File;
$fh->open(">$TMP_LOG_FILE") or die "FATAL: can't write to temp file $TMP_LOG_FILE, $!\n";
$fh->print();
$fh->close();

# Set remote PostgreSQL related options
if ($REMOTE_DBNAME) {
	$REMOTE_PG_OPT .= " -d $REMOTE_DBNAME";
}
if ($REMOTE_DBHOST) {
	$REMOTE_PG_OPT .= " -h $REMOTE_DBHOST";
}
if ($REMOTE_DBPORT) {
	$REMOTE_PG_OPT .= " -p $REMOTE_DBPORT";
}
if ($REMOTE_DBUSER) {
	$REMOTE_PG_OPT .= " -U $REMOTE_DBUSER";
}

# Set DBD::Pg options and connect to the database for testing
my $dbpg_opt = '';
$dbpg_opt .= ";port=$DBPORT" if ($DBPORT);
$dbpg_opt .= ";host=$DBHOST" if ($DBHOST);
$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
if (not defined $dbh) {
	logmsg('FATAL', "cannot connect to database $DBNAME");
}

my $remote_dbpg_opt = '';
$remote_dbpg_opt .= ";port=$REMOTE_DBPORT" if ($REMOTE_DBPORT);
$remote_dbpg_opt .= ";host=$REMOTE_DBHOST" if ($REMOTE_DBHOST);

# Look at PostgreSQL version
my $sth = $dbh->prepare("SELECT version()") or die "FATAL: " . $dbh->errstr . "\n";
$sth->execute or die "FATAL: " . $dbh->errstr . "\n";
my $pgversion = 0;
while (my $row = $sth->fetch)
{
	if ($row->[0] =~ /^[^\s]+ ([\.\d]+)/) {
		$pgversion = $1;
	}
}
$sth->finish;
logmsg('LOG', "Connection to PostgreSQL successfull");

logmsg('LOG', 'Starting conversion of large objects to bytea');

# Get oid columns to migrate
logmsg('LOG', 'Looking for large objects columns');
my $collist_query = "SELECT n.nspname, c.relname, a.attname FROM pg_attribute a JOIN pg_class c ON (a.attrelid=c.oid) JOIN pg_namespace n ON (c.relnamespace=n.oid) WHERE a.atttypid = 26 AND a.attnum > 0 AND n.nspname NOT IN ('pg_catalog', 'pg_toast', 'information_schema') AND NOT EXISTS (SELECT 1 FROM pg_catalog.pg_depend d WHERE d.refclassid = 'pg_catalog.pg_extension'::pg_catalog.regclass AND d.objid = c.oid AND d.deptype = 'e')";
if ($#INC_SCHEMAS < 0)
{
	$collist_query .= " AND n.nspname NOT IN ('pg_catalog', 'pg_toast', 'information_schema') AND NOT EXISTS (SELECT 1 FROM pg_catalog.pg_depend d WHERE d.refclassid = 'pg_catalog.pg_extension'::pg_catalog.regclass AND d.objid = c.oid AND d.deptype = 'e')";
}
else
{
	$collist_query .= "  AND n.nspname IN ('" . join("','", @INC_SCHEMAS) . "')";
}
$collist_query .= " ORDER BY a.attnum";

#logmsg('LOG', "Using query: $collist_query");

$sth = $dbh->prepare($collist_query) or die "FATAL: " . $dbh->errstr . "\n";
$sth->execute or die "FATAL: " . $dbh->errstr . "\n";

# Query to get the full coilumn list of a table
my $allcol_query = "SELECT a.attname FROM pg_attribute a JOIN pg_class c ON (a.attrelid=c.oid) JOIN pg_namespace n ON (c.relnamespace=n.oid) WHERE n.nspname = ? AND c.relname = ? AND a.attnum > 0 ORDER BY a.attnum";
my $sth2 = $dbh->prepare($allcol_query) or die "FATAL: " . $dbh->errstr . "\n";

my %collist = ();
my %oid_collist = ();
while (my $row = $sth->fetch)
{
	logmsg('LOG', "Found oid column in table $row->[0].$row->[1] column $row->[2].");
	if ($#INC_TABLES < 0 || grep(/^$row->[1]$/, @INC_TABLES) || grep(/^$row->[0].$row->[1]$/, @INC_TABLES))
	{
		logmsg('LOG', "Registering column \"$row->[2]\" from table \"$row->[0].$row->[1]\" for migration.");
		push(@{ $oid_collist{"$row->[0].$row->[1]"} }, $row->[2]);
		# Grab the full list of column for remote work
		if ($allcol_query)
		{
			$sth2->execute($row->[0], $row->[1]) or die "FATAL: " . $dbh->errstr . "\n";
			while (my $r = $sth2->fetch)
			{
				push(@{ $collist{"$row->[0].$row->[1]"} }, $r->[0]);
			}
		}
	}
	else
	{
		logmsg('LOG', "Skiping, not in table list.");
	}
}
$sth->finish;
$sth2->finish;

# Disconnect from snapshot connection
$dbh->disconnect;

if (scalar keys %oid_collist == 0) {
	logmsg('LOG', "No large object columns found.");
}

my $procnum = 0;
foreach my $tbname (sort keys %collist)
{
	last if ($interrupt);

	spawn sub {
		if (!$REMOTE_DBNAME) {
			logmsg('LOG', "Processing table $tbname locally...");
			transform_oid_to_bytea($procnum, $tbname, @{ $oid_collist{$tbname} });
		} else {
			logmsg('LOG', "Processing table $tbname remotely...");
			remote_transform_oid_to_bytea($procnum, $tbname, \@{ $collist{$tbname} }, \@{ $oid_collist{$tbname} });
		}
	};
	$procnum++;

	while ($procnum >= $JOBS)
	{
		my $kid = waitpid(-1, WNOHANG);
		if ($kid > 0)
		{
			$procnum--;
			delete $RUNNING_PIDS{$kid};
			last;
		}
		usleep(50000);
	}
}

# Then wait for all child processes to die
while (scalar keys %RUNNING_PIDS > 0)
{
        my $kid = waitpid(-1, WNOHANG);
        if ($kid > 0)
        {
                delete $RUNNING_PIDS{$kid};
        }
        usleep(50000);
}

# Now vacuum all orphan large object from the database
if (scalar keys %oid_collist > 0)
{
	logmsg('LOG', "Running vacuumlo on the database to remove orphan large object");
	if ($DRY_RUN) {
		logmsg('LOG', "vacuumlo --dry-run $PG_OPT $DBNAME");
	} else {
		`vacuumlo $PG_OPT $DBNAME`;
	}
}

if ($interrupt)
{
	logmsg('LOG', '');
	unlink($TMP_LOG_FILE);
	_exit(1);
}

logmsg('LOG', 'convertion ended');

unlink($TMP_LOG_FILE);

exit 0;

#---------------------------------------------------------------------

####
# Show help about the program
####
sub usage
{
        my $msg = shift();

        print qq{
Program used to convert large objects columns in a PostgreSQL database into
bytea. Two migration modes are supported: local and remote.

**local**: the corresponding large objects data stored in the pg_largeobject table
will be moved into a newly created bytea column and the old oid column will be
removed. If no table list to convert is provided all columns with the oid data
type will be converted to bytea otherwise all oid columns of the table list
will be converted to bytea. 

	ALTER TABLE tb1 ADD COLUMN bytea_col bytea;
	UPDATE tb1 SET bytea_col = lo_get(lo_col);
	ALTER TABLE tb1 DROP COLUMN lo_col;
	ALTER TABLE tb1 rename column bytea_col TO lo_col;

Once all large objects are migrated the tool runs the vacuumlo command to
remove all orphan large objects from the database.

WARNING: the large object data will be duplicated until the `vacuumlo` command
is run, be sure to have enough free space.

You may want to run a VACUUM FULL on modified tables to recover space from
the oid column(s).

Also take care in your application that the bytea column(s) are appended at end
of the table so their attribute position number change.

**remote**:
in this mode oid2bytea will migrate the local oid column to a remote database
with the same structure except that the oid column have been replaced by a bytea
column of the same name. To use the remote mode you just have to set the remote
command line option (-D, -H, -P, -U). The all the data of the table with the oid
column(s) will be moved and the oid replaced by the content of the large object
before being sent to the remote database.


**multiprocess**:
The two modes can benefit of high speed migration by parallelizing tables
migration using the `-j` option. With table with huge amount of rows it is also
possible to parallelize rows export of a single table by using the `-J` option.
The value for these option is the number of processes/CPUs you want to use to
process the data.  Take care that the resulting number of processes/CPUs used
is `-j * -J`.

Usage: oid2bytea -d dbname [options]

options:

  -d, --database DBNAME      database where the migration job must be done.
  -D, --dest-database DBNAME database where the migration data must be sent.
  -f, --function FCTNAME     use this function to process the bytea returned
                             by the call to lo_get(). It must return a bytea.
  -h, --host HOSTNAME        database server host or socket directory.
  -H, --dest-host HOSTNAME   remote database server host or socket directory.
  -j, --job NUM              use this many parallel jobs to process all tables.
  -J, --job-split NUM        use this many parallel jobs to process single table
  -n, --namespace NAME       process tables created under the given schema.
                             Can be used multiple time.
  -p, --port PORT            database server port number, default: 5432.
  -P, --dest-port PORT       database server port number, default: 5432.
  -q, --quiet                don't display messages issued with the LOG level.
  -t, --table TABLE          migrate oid column(s) of the named relation. Can
                             be used multiple time.
  -u, --user NAME            connect as specified database user.
  -U, --dest-user NAME       remote connect as specified database user.
  -v, --version              show program version.
  -V, --no-vacuumlo          do not run vacuumlo at end to remove orphan large
                             objects.
  --help                     show usage.
  --no-time                  don't show timestamp in log output.
  --dry-run                  do not do anything, just show what will be done.

$msg
};
	exit 0;
}

####
# Return the date in ISO 8601 format minus the timezone part.
# As an example of returned value: 2019-09-03T09:03:12
####
sub get_current_date
{
    return strftime("%FT%T", localtime);
}

####
# Print a message with date/time and log level.
####
sub logmsg
{
	my ($level, $message) = @_;

	return if ($QUIET);

	$fh = new IO::File;
	$fh->open(">>$TMP_LOG_FILE") or die "FATAL: can't write to temp file $TMP_LOG_FILE, $!\n";
	flock($fh, 2) || die "FATAL: can't lock file $TMP_LOG_FILE\n";

	if ($NOTIME) {
		print "$level: $message\n";
	} else {
		print '[' , get_current_date(), '] ', "$level: $message\n";
	}

	$fh->close;
}


sub transform_oid_to_bytea
{
	my ($proc, $tb, @colums) = @_;

	return if ($#colums < 0);

	$dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
	if (not defined $dbh) {
		logmsg('FATAL', "cannot connect to database $DBNAME");
	}

	# Construct the SQL orders to execute
	my @alter1 = ();
	my @alter2 = ();
	my $update_sql = "UPDATE $tb SET ";
	for (my $c = 0; $c <= $#colums; $c++)
	{
	
		push(@alter1, "ALTER TABLE $tb ADD COLUMN bytea_col$c bytea");
		$update_sql .= "bytea_col$c = ";
		if ($FUNCTION) {
			$update_sql .= "$FUNCTION(lo_get($colums[$c])), ";
		} else {
			$update_sql .= "lo_get($colums[$c]), ";
		}
		push(@alter2, "ALTER TABLE $tb DROP COLUMN $colums[$c]");
		push(@alter2, "ALTER TABLE $tb RENAME COLUMN bytea_col$c TO $colums[$c]");
	}
	$update_sql =~ s/, $//;

	# Add the bytea column(s)
	foreach my $q (@alter1)
	{
		logmsg('LOG', $q);
		if (!$DRY_RUN) {
			$dbh->do($q) or logmsg('ERROR', $dbh->errstr);
		}
	}

	# Migrate the large object into the bytea column(s)
	if ($SPLIT > 1)
	{
		my $procnum = 0;
		for my $splitnum (1 .. $SPLIT)
		{
			logmsg('LOG', "Processing table $tb on chunk $splitnum on column $colums[0]...");
			spawn sub {
				update_column($update_sql, $colums[0], $splitnum);
			};
			$procnum++;
		}
		while ($procnum > 0)
		{
			my $kid = waitpid(-1, WNOHANG);
			if ($kid > 0)
			{
				$procnum--;
				delete $RUNNING_PIDS{$kid};
				last;
			}
			usleep(50000);
		}
	}
	else
	{
		update_column($update_sql, '', 0);
	}

	# Drop the oid column(s) and rename the bytea column(s) with the old name of the old oid column(s)
	foreach my $q (@alter2)
	{
		logmsg('LOG', $q);
		if (!$DRY_RUN) {
			$dbh->do($q) or logmsg('ERROR', $dbh->errstr);
		}
	}

	$dbh->disconnect();
}

sub remote_transform_oid_to_bytea
{
	my ($proc, $tb, $all_columns, $oid_colums) = @_;

	return if ($#{$oid_colums} < 0);

	my $query_get = '';
	foreach my $c (@$all_columns)
	{
		if (!grep(/^$c$/, @$oid_colums)) {
			$query_get .= "$c, ";
		} else {
			if ($FUNCTION) {
				$query_get .= "$FUNCTION(lo_get($c)), ";
			} else{
				$query_get .= "lo_get($c), ";
			}
		}
	}
	$query_get =~ s/, $//;
	$query_get .= ' FROM ' . $tb;

	$query_get .= " WHERE MOD($oid_colums->[0]::bigint, $SPLIT) = $procnum - 1" if ($procnum > 0 and $oid_colums->[0]);
	$query_get = qq{COPY (SELECT $query_get) TO PROGRAM 'psql -X $REMOTE_PG_OPT -c "\\\\copy $tb FROM stdin"'};

	# Construct the SQL query to get data
	if ($SPLIT > 1)
	{
		my $procnum = 0;
		for my $splitnum (1 .. $SPLIT)
		{
			logmsg('LOG', "Processing table $tb on chunk $splitnum on column $oid_colums->[0]...");
			spawn sub {
				forward_data($query_get, $splitnum);
			};
			$procnum++;
		}
		while ($procnum > 0)
		{
			my $kid = waitpid(-1, WNOHANG);
			if ($kid > 0)
			{
				$procnum--;
				delete $RUNNING_PIDS{$kid};
				last;
			}
			usleep(50000);
		}
	}
	else
	{
		forward_data($query_get, 0);
	}
}


sub update_column
{
	my ($update_sql, $split_col, $procnum) = @_;

	$update_sql .= " WHERE MOD(${split_col}::bigint, $SPLIT) = $procnum - 1" if ($procnum > 0 and $split_col);
	logmsg('LOG', $update_sql);
	if (!$DRY_RUN) {
		$dbh->do($update_sql) or logmsg('FATAL', $dbh->errstr);
	}
}

sub forward_data
{
	my ($sql_get, $procnum) = @_;

	logmsg('LOG', $sql_get);
	if (!$DRY_RUN)
	{
		my $dbh = DBI->connect("dbi:Pg:application_name=oid2bytea;dbname=$DBNAME$dbpg_opt", $DBUSER, $DBPWD, {AutoCommit => 1, InactiveDestroy => 1});
		if (not defined $dbh) {
			logmsg('FATAL', "cannot connect to database $DBNAME");
		}
		$dbh->do($sql_get);
		$dbh->disconnect();
	}
}
